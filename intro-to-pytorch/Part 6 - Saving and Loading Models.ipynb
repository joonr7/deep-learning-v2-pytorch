{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Models\n",
    "\n",
    "In this notebook, I'll show you how to save and load models with PyTorch. This is important because you'll often want to load previously trained models to use in making predictions or to continue training on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper\n",
    "# 새로운거 fully connected 모델\n",
    "import fc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "# 데이터 표준화 위한 변환을 정의\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "# 다운로드 train dataset\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "# 다운로드 test dataset. train옵션이 False가 돼 있다.\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 231,
       "width": 231
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a network\n",
    "\n",
    "To make things more concise here, I moved the model architecture and training code from the last part to a file called `fc_model`. Importing this, we can easily create a fully-connected network with `fc_model.Network`, and train the network using `fc_model.train`. I'll use this model (once it's trained) to demonstrate how we can save and load models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, define the criterion and optimizer\n",
    "\n",
    "# 입력 784 (28x28), 출력 10, 512, 256, 128 은 중간 히든 레이어들 노드(유닛) 수.\n",
    "# 아까 import한 fc_model은 이렇게 쓴다.\n",
    "model = fc_model.Network(784, 10, [512, 256, 128])\n",
    "\n",
    "# Loss함수로는 Negative Logarithm Likelihood\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# 옵티마이저는 아담, lr은 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Training Loss: 1.681..  Test Loss: 0.985..  Test Accuracy: 0.635\n",
      "Epoch: 1/2..  Training Loss: 1.023..  Test Loss: 0.756..  Test Accuracy: 0.714\n",
      "Epoch: 1/2..  Training Loss: 0.878..  Test Loss: 0.681..  Test Accuracy: 0.747\n",
      "Epoch: 1/2..  Training Loss: 0.783..  Test Loss: 0.654..  Test Accuracy: 0.749\n",
      "Epoch: 1/2..  Training Loss: 0.768..  Test Loss: 0.622..  Test Accuracy: 0.759\n",
      "Epoch: 1/2..  Training Loss: 0.683..  Test Loss: 0.605..  Test Accuracy: 0.773\n",
      "Epoch: 1/2..  Training Loss: 0.726..  Test Loss: 0.601..  Test Accuracy: 0.778\n",
      "Epoch: 1/2..  Training Loss: 0.641..  Test Loss: 0.580..  Test Accuracy: 0.785\n",
      "Epoch: 1/2..  Training Loss: 0.672..  Test Loss: 0.556..  Test Accuracy: 0.794\n",
      "Epoch: 1/2..  Training Loss: 0.606..  Test Loss: 0.542..  Test Accuracy: 0.797\n",
      "Epoch: 1/2..  Training Loss: 0.640..  Test Loss: 0.521..  Test Accuracy: 0.805\n",
      "Epoch: 1/2..  Training Loss: 0.638..  Test Loss: 0.533..  Test Accuracy: 0.798\n",
      "Epoch: 1/2..  Training Loss: 0.630..  Test Loss: 0.519..  Test Accuracy: 0.813\n",
      "Epoch: 1/2..  Training Loss: 0.613..  Test Loss: 0.509..  Test Accuracy: 0.810\n",
      "Epoch: 1/2..  Training Loss: 0.571..  Test Loss: 0.515..  Test Accuracy: 0.811\n",
      "Epoch: 1/2..  Training Loss: 0.560..  Test Loss: 0.506..  Test Accuracy: 0.815\n",
      "Epoch: 1/2..  Training Loss: 0.576..  Test Loss: 0.524..  Test Accuracy: 0.811\n",
      "Epoch: 1/2..  Training Loss: 0.638..  Test Loss: 0.494..  Test Accuracy: 0.819\n",
      "Epoch: 1/2..  Training Loss: 0.582..  Test Loss: 0.513..  Test Accuracy: 0.811\n",
      "Epoch: 1/2..  Training Loss: 0.547..  Test Loss: 0.483..  Test Accuracy: 0.825\n",
      "Epoch: 1/2..  Training Loss: 0.560..  Test Loss: 0.467..  Test Accuracy: 0.831\n",
      "Epoch: 1/2..  Training Loss: 0.561..  Test Loss: 0.488..  Test Accuracy: 0.821\n",
      "Epoch: 1/2..  Training Loss: 0.634..  Test Loss: 0.480..  Test Accuracy: 0.823\n",
      "Epoch: 2/2..  Training Loss: 0.552..  Test Loss: 0.473..  Test Accuracy: 0.827\n",
      "Epoch: 2/2..  Training Loss: 0.571..  Test Loss: 0.494..  Test Accuracy: 0.819\n",
      "Epoch: 2/2..  Training Loss: 0.544..  Test Loss: 0.460..  Test Accuracy: 0.833\n",
      "Epoch: 2/2..  Training Loss: 0.549..  Test Loss: 0.480..  Test Accuracy: 0.823\n",
      "Epoch: 2/2..  Training Loss: 0.529..  Test Loss: 0.481..  Test Accuracy: 0.829\n",
      "Epoch: 2/2..  Training Loss: 0.526..  Test Loss: 0.472..  Test Accuracy: 0.826\n",
      "Epoch: 2/2..  Training Loss: 0.529..  Test Loss: 0.497..  Test Accuracy: 0.815\n",
      "Epoch: 2/2..  Training Loss: 0.564..  Test Loss: 0.471..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.527..  Test Loss: 0.452..  Test Accuracy: 0.832\n",
      "Epoch: 2/2..  Training Loss: 0.516..  Test Loss: 0.453..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.495..  Test Loss: 0.465..  Test Accuracy: 0.831\n",
      "Epoch: 2/2..  Training Loss: 0.533..  Test Loss: 0.465..  Test Accuracy: 0.827\n",
      "Epoch: 2/2..  Training Loss: 0.520..  Test Loss: 0.452..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.497..  Test Loss: 0.449..  Test Accuracy: 0.840\n",
      "Epoch: 2/2..  Training Loss: 0.532..  Test Loss: 0.459..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.499..  Test Loss: 0.454..  Test Accuracy: 0.836\n",
      "Epoch: 2/2..  Training Loss: 0.517..  Test Loss: 0.443..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.523..  Test Loss: 0.457..  Test Accuracy: 0.833\n",
      "Epoch: 2/2..  Training Loss: 0.538..  Test Loss: 0.445..  Test Accuracy: 0.839\n",
      "Epoch: 2/2..  Training Loss: 0.525..  Test Loss: 0.454..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.552..  Test Loss: 0.477..  Test Accuracy: 0.821\n",
      "Epoch: 2/2..  Training Loss: 0.488..  Test Loss: 0.432..  Test Accuracy: 0.843\n",
      "Epoch: 2/2..  Training Loss: 0.515..  Test Loss: 0.451..  Test Accuracy: 0.835\n"
     ]
    }
   ],
   "source": [
    "fc_model.train(model, trainloader, testloader, criterion, optimizer, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading networks\n",
    "\n",
    "As you can imagine, it's impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions.\n",
    "\n",
    "The parameters for PyTorch networks are stored in a model's `state_dict`. We can see the state dict contains the weight and bias matrices for each of our layers.\n",
    "\n",
    "PyTorch 네트워크의 파라미터는 모델의 state_dict안에 저장된다. state dict는 각 레이어마다의 weight랑 bias 행렬을 저장하고 있다. 사전형이니까 키값이랑 밸류값 가지고 있겠지? 아래 셀 참조."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model: \n",
      "\n",
      " Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ") \n",
      "\n",
      "The state dict keys: \n",
      "\n",
      " odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our model: \\n\\n\", model, '\\n')\n",
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest thing to do is simply save the state dict with `torch.save`. For example, we can save it to a file `'checkpoint.pth'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 심플하게 저장하는 방법: torch.save. 위에서봤듯, state_dict는 각 레이어의 웨이트바이어스 행렬\n",
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can load the state dict with `torch.load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "# 불러오기 방법은 torch.load\n",
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to load the state dict in to the network, you do `model.load_state_dict(state_dict)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 내가 아까 fc_model써서 정의한 model을 가지고\n",
    "# 지금 checkpoint.pth에서 읽어들인 state_dict 값 쓰고 싶을 때 아래와 같이게 쓰면 된다.\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty straightforward, but as usual it's a bit more complicated. Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture. If I create a model with a different architecture, this fails.\n",
    "\n",
    "모델 레이어 유닛 숫자 다르거나 하면 에러 뜬다. 당연한이야기지만."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d859c59ebec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# This will throw an error because the tensor sizes are wrong!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda/envs/deep-learning/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 847\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    848\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100])."
     ]
    }
   ],
   "source": [
    "# Try this\n",
    "model = fc_model.Network(784, 10, [400, 200, 100])\n",
    "# This will throw an error because the tensor sizes are wrong!\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we need to rebuild the model exactly as it was when trained. Information about the model architecture needs to be saved in the checkpoint, along with the state dict. To do this, you build a dictionary with all the information you need to compeletely rebuild the model.\n",
    "\n",
    "당연한 방법이지만, checkpoint.pth 저장할 때, 그냥 무작정 웨이트랑 바이어스 값만 저장하는 게 아니라, Neural Network의 레이어 구조도 같이 저장해 주면 좋겠지? 그래서 아래와 같이 하면 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the checkpoint has all the necessary information to rebuild the trained model. You can easily make that a function if you want. Similarly, we can write a function to load checkpoints. \n",
    "\n",
    "약간 귀찮을 수 있겠지만, 아래와 같이 함수 만들어주면, 저장한거 그대로 불러오기 편하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = fc_model.Network(checkpoint['input_size'],\n",
    "                             checkpoint['output_size'],\n",
    "                             checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
      "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint('checkpoint.pth')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
